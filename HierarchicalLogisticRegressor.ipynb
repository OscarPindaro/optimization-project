{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "531529ed",
   "metadata": {},
   "source": [
    "# Hiercarchical Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2861e4f2",
   "metadata": {},
   "source": [
    "This notebook showcases the *HiercarchicalLogisticRegression* that I made. This class computes the initialization for the ORCT, in the hope that it will yield to better results or faster convergence.\n",
    "The work can be divided into 3 steps. In the first step the dataset is partitioned into clusters. In the second step, each cluster is assigned to a leaf in the ORCT model maximizing some \"purity\" metric. In the third step, a **Hierarchical Loogistic Regressor** is trained in order to force the ORCT to assign each sample to the corrensponding leaf."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87176f6c",
   "metadata": {},
   "source": [
    "### Preprocessing of the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba292db3",
   "metadata": {},
   "source": [
    "This part loads a dataset in order to have some data on which run the algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f34fd56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.cluster import KMeans, SpectralClustering, AgglomerativeClustering, Birch\n",
    "from sklearn.metrics import completeness_score, homogeneity_score\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import operator\n",
    "from pyomo.environ import *\n",
    "from pyomo.opt import SolverFactory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80c20784",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "name = \"splice.csv\"\n",
    "DATASET_PATH = os.path.join(\"datasets\", name)\n",
    "df = pd.read_csv(DATASET_PATH, delimiter=\";\", header=0)\n",
    "columns = list(df.columns)\n",
    "X = df[columns[:-1]]\n",
    "y = df[columns[-1]]\n",
    "feature_names = columns[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82d323d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 1594\n",
      "Shape: (1594, 187)\n",
      "The are 187 columns\n",
      "\n",
      "Distinct values for 'Classes' column\n",
      "2    834\n",
      "0    384\n",
      "1    376\n",
      "Name: Classes, dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of rows: {}\\nShape: {}\".format(len(df), df.shape))\n",
    "print(\"The are {} columns\".format(len(df.columns)))\n",
    "print(\"\\nDistinct values for 'Classes' column\\n{}\\n\".format(df[\"Classes\"].value_counts()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ed71869",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "vals = y.unique()\n",
    "vals.sort()\n",
    "heights = [len(y[y==x]) for x in vals ]\n",
    "vals = [str(x) for x in vals]\n",
    "plt.bar(vals, heights)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd87fc8d",
   "metadata": {},
   "source": [
    "The class 2 is more present than the other two classes, but the dataset can be considered fairly balanced."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18663802",
   "metadata": {},
   "source": [
    "## Find the best clustering\n",
    "In order to find the parameters of the ORCT, we need to partition the dataset in different clusters. The number of leaves of the ORCT depends on its depth and it's always a power of two, since the ORCT is a binary tree. In this dataset we have 3 classes, and the closest power of 2 is 4, therefore we will have a 2-depth tree.\n",
    "Three possible approaches are possible here:\n",
    "* An unsupervised algorithm that outputs exatcly **n_leaves** cluster.\n",
    "* An unsupervised algorithm that has no gaurantee on the number of cluster found.\n",
    "* The original class division.\n",
    "\n",
    "In the first case, an algorithm such as *KMeans* can be used. The advantage of this approach is that every leaf will have a cluster and therefore every branch node will participate in the classification task.The main drawback is that if the dataset has a lower number of classes, the division in exactly **n_leaves** cluster may be not effective since that kind of structure is not present in the data.<br>\n",
    "In the second case, we just need to ensure that the number of cluster found is les than the number of wanted leaves. Since there is no gaurantee on the number of clusters found, this algorithm could output a number much smaller of **n_leaves**, maybe even smaller than of **n_classes**, and the ORCT will probably not perform really well. <br>\n",
    "In the last case, we can use the original class division. In these way, no assumption on the input data is done and all the members of the same class are assigned to the same leaf. The drawback is that we are using less leaves in the case $n_{classes} < n_{leaves} $.<br>\n",
    "The quality of a solution is measured with a **purity** metric, that tells how similar are the data points present in a cluster. This metric assumes known the original class of each sample. Since this is a classification task, we have this knowledge. <br>\n",
    "I decided to use the **homogeneity score** $hs$. A clustering is very homogeneus if all of its clusters contain samples which are member of the same class. Note that $hs \\in [0,1]$ . Values near one imply a very homogeneous clustering result. <br>\n",
    "The following code uses some clustering algorithm that ensure to have **n_leaves** clusters. A solution with the original labeling is also used and compared to the clustering algorithm.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23a1df1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_std = X.copy()\n",
    "X_std[feature_names] = StandardScaler().fit_transform(X[feature_names])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_std, y, test_size=0.25, random_state=SEED)\n",
    "classes = y.unique().tolist()\n",
    "classes.sort() # sorted    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da06133",
   "metadata": {},
   "source": [
    "Sample weigthing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "630a2811",
   "metadata": {},
   "outputs": [],
   "source": [
    "occurences = [len(y_train[y_train==x]) for x in classes]\n",
    "total_samples = sum(occurences)\n",
    "sample_weight = np.zeros_like(y_train)\n",
    "for class_index, n_occurr in zip(classes, occurences):\n",
    "    sample_weight[y_train==class_index]=n_occurr\n",
    "sample_weight = sample_weight/total_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d272481c",
   "metadata": {},
   "source": [
    "The clustering estimators used are *Kmeans, Spectral Clustering, Agglomerative Clustering (with 4 different linkage types), and Birch*. All the clustering estimators are put in the same list *clustering_estimators*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b294dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_leaves = 4\n",
    "n_clusters = n_leaves\n",
    "clustering_estimators = []\n",
    "params = dict(n_clusters=n_clusters, random_state=SEED)\n",
    "kmeans = KMeans(**params)\n",
    "clustering_estimators.append(kmeans)\n",
    "\n",
    "# Spectral clustering not used since it gave looped\n",
    "\n",
    "params = dict(n_clusters=n_clusters, linkage=\"single\")\n",
    "agglomerate = AgglomerativeClustering(**params)\n",
    "clustering_estimators.append(agglomerate)\n",
    "\n",
    "params = dict(n_clusters=n_clusters, linkage=\"ward\")\n",
    "agglomerate = AgglomerativeClustering(**params)\n",
    "clustering_estimators.append(agglomerate)\n",
    "\n",
    "params = dict(n_clusters=n_clusters, linkage=\"complete\")\n",
    "agglomerate = AgglomerativeClustering(**params)\n",
    "clustering_estimators.append(agglomerate)\n",
    "\n",
    "params = dict(n_clusters=n_clusters, linkage=\"average\")\n",
    "agglomerate = AgglomerativeClustering(**params)\n",
    "clustering_estimators.append(agglomerate)\n",
    "\n",
    "params = dict(n_clusters=n_clusters)\n",
    "birch = Birch(**params)\n",
    "clustering_estimators.append(birch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed5218a",
   "metadata": {},
   "source": [
    "Fit every estimator and computes their homogeneity. In the code below the clustering algorithm that support *sample_weight* are instantiated with it. <br>\n",
    "The homogeneity score is computed for each fitted estimator.The **find_best_estimator** function is used to find the best estimator given a certain metric. It assumes that the metric needs the real labels of the data, so that the original information embedded in the data points can be used. <br>\n",
    "If the original labelling is used, its homogeneity would be of couse 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6743c692",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/oscarpindaro/git_projects/optimization-project/src/cluster.py:271: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if self.prediction_type is \"deterministic\":\n",
      "/home/oscarpindaro/git_projects/optimization-project/src/cluster.py:273: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  elif self.prediction_type is \"probabilistic\":\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KMeans 0.27684999814353056\n",
      "AgglomerativeClustering 0.0022374154879104663\n",
      "AgglomerativeClustering 0.10784173404673172\n",
      "AgglomerativeClustering 0.0022374154879104663\n",
      "AgglomerativeClustering 0.0022374154879104663\n",
      "Birch 0.13976558265285352\n",
      "The best estimator is KMeans(n_clusters=4, random_state=1234)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/oscarpindaro/miniconda3/envs/decision_trees/lib/python3.9/site-packages/sklearn/base.py:445: UserWarning: X does not have valid feature names, but Birch was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from src.cluster import find_best_estimator\n",
    "\n",
    "for i in range(len(clustering_estimators)):\n",
    "    try:\n",
    "        clustering_estimators[i] = clustering_estimators[i].fit(X_train, sample_weight=sample_weight)\n",
    "    except:\n",
    "        clustering_estimators[i] = clustering_estimators[i].fit(X_train)\n",
    "\n",
    "for estimator in clustering_estimators:\n",
    "    print(estimator.__class__.__name__, homogeneity_score(y_train, estimator.labels_))\n",
    "    \n",
    "best_estimator = find_best_estimator(clustering_estimators, homogeneity_score, y_train)\n",
    "print(\"The best estimator is {}\".format(best_estimator))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f005c391",
   "metadata": {},
   "source": [
    "## Find the best leaves assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518e5f05",
   "metadata": {},
   "source": [
    "Now we sould find the best leaves assignment. Now that we have divided the data in $n_{leaves}$ cluster, we should decide  which cluster is assigned to which leaf. The idea is to find an order such that, when two cluster are merged, a certain metric sould be maximized. This procedure is repeated recursively till only two mega-clusters with maximum metric value are found. <br>\n",
    "This assignment is computed with a Depth-First Search Algorithm that checks every possible coupling. Since inside a couple the order of the clusters does not matter, and since there is no need to check permutations of couples for the same reason, the number of possible coupling is: <br>\n",
    "$ {n \\choose 2} {n-2 \\choose 2} ... {4 \\choose 2} / \\frac n 2! = (n-1)*(n-3)...5*3$ <br>\n",
    "**NB**: I think this formula is correct only for $n<8$, but i'm not super sure, probably it's a lower bound. <br><br>\n",
    "Here the *completeness_score* is used. A clustering assignment satisfies completeness if all the data points of a given class belong to the same cluster. The closer the completeness is to 1, the more complete the clustering result. In this case, it looks like the best complete estimator was Birch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1dfe70a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the estimator KMeans, the assignment [0, 1, 2, 3] has a score of 0.18260695983039052\n",
      "For the estimator AgglomerativeClustering, the assignment [0, 3, 1, 2] has a score of 0.08778700261653409\n",
      "For the estimator AgglomerativeClustering, the assignment [0, 2, 1, 3] has a score of 0.18418527760037742\n",
      "For the estimator AgglomerativeClustering, the assignment [0, 3, 1, 2] has a score of 0.08778700261653409\n",
      "For the estimator AgglomerativeClustering, the assignment [0, 3, 1, 2] has a score of 0.08778700261653409\n",
      "For the estimator Birch, the assignment [0, 2, 1, 3] has a score of 0.26478933557964085\n"
     ]
    }
   ],
   "source": [
    "from src.cluster import best_leaf_assignment\n",
    "for estimator in clustering_estimators:\n",
    "    assignment, score = best_leaf_assignment(n_leaves=n_leaves, estimated_labels=estimator.labels_, \n",
    "                                      true_labels=y_train, metric=completeness_score)\n",
    "    print(\"For the estimator {}, the assignment {} has a score of {}\".format(estimator.__class__.__name__,assignment, score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c78383",
   "metadata": {},
   "source": [
    "The true labelling will be of course complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d6df14e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The true labelling has assignment [0, 1, 2, 3] with score 1.0000000000000002\n"
     ]
    }
   ],
   "source": [
    "assignment, score = best_leaf_assignment(n_leaves=n_leaves, estimated_labels=y_train, \n",
    "                                      true_labels=y_train, metric=completeness_score)\n",
    "print(\"The true labelling has assignment {} with score {}\".format(assignment, score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ee50ce",
   "metadata": {},
   "source": [
    "## Parameters Initialization\n",
    "Now that the best estimator was found and the best leaf assignment was computed, we need to train the **Hierarchical Logistic Regressor**. This classifier is made of multiple logistic regressors, structured in a binary tree fashion. Every regressor is trained against a binary classification problem. If the label is zero, that means that the leaf that contains that sample is in the left subtree, otherwhise it's in the right subtree. If the tree has $n_{leaves}$, the number of logistic regressors that have to be trained are $n_{leaves}-1$. Every problem is independent, since the classification of a parent node has no effect on the classification of its childs. This is because all nodes know in advance where each sample belongs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "39e376f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.cluster import HierarchicalLogisticRegression\n",
    "HLR = HierarchicalLogisticRegression(n_classes=len(np.unique(y_train)), n_leaves=n_leaves, prediction_type=\"deterministic\", random_state=0,\n",
    "                                     logistic_params={\"class_weight\": \"balanced\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fabbdf2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KMeans(n_clusters=4, random_state=1234) accuracy:0.07017543859649122\n",
      "AgglomerativeClustering(linkage='single', n_clusters=4) accuracy:0.5238095238095238\n",
      "AgglomerativeClustering(n_clusters=4) accuracy:0.706766917293233\n",
      "AgglomerativeClustering(linkage='complete', n_clusters=4) accuracy:0.5238095238095238\n",
      "AgglomerativeClustering(linkage='average', n_clusters=4) accuracy:0.5238095238095238\n",
      "Birch(n_clusters=4) accuracy:0.706766917293233\n",
      "The best was AgglomerativeClustering(n_clusters=4) with score 0.706766917293233\n"
     ]
    }
   ],
   "source": [
    "best = clustering_estimators[0]\n",
    "best_accuracy = 0\n",
    "i = 0\n",
    "for estimator in clustering_estimators:\n",
    "    \"\"\"\n",
    "    print(estimator)\n",
    "    print(np.unique(estimator.labels_))\n",
    "    for un in np.unique(estimator.labels_):\n",
    "        print(len(estimator.labels_[estimator.labels_==un]))\n",
    "    \"\"\"\n",
    "    assignment, score = best_leaf_assignment(n_leaves=n_leaves, estimated_labels=estimator.labels_, \n",
    "                                  true_labels=y_train, metric=completeness_score)\n",
    "    HLR = HLR.fit(X_train.to_numpy(), y_train, cluster_labels=estimator.labels_, leaves_assignment=assignment)\n",
    "    accuracy = HLR.score(X_test.to_numpy(), y_test)\n",
    "    print(\"{} accuracy:{}\".format(estimator, accuracy))\n",
    "    if accuracy > best_accuracy:\n",
    "        best = clustering_estimators[i]\n",
    "        best_accuracy = accuracy\n",
    "    i +=1\n",
    "print(\"The best was {} with score {}\".format(best, best_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947c3db5",
   "metadata": {},
   "source": [
    "Now let's try to use the real labeling. It has much higher performances, because it has the most homogenous clustering and also the most complete. However, this may stuck the optimization of the ORCT in a local minima. Some tests should still be made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d2e542b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The true labelling has assignment [0, 1, 2, 3] with score 1.0000000000000002\n",
      "Accuracy using true labellling: 0.8796992481203008\n"
     ]
    }
   ],
   "source": [
    "assignment, score = best_leaf_assignment(n_leaves=n_leaves, estimated_labels=y_train, \n",
    "                                      true_labels=y_train, metric=completeness_score)\n",
    "print(\"The true labelling has assignment {} with score {}\".format(assignment, score))\n",
    "HLR = HierarchicalLogisticRegression(n_classes=len(np.unique(y_train)),\n",
    "                                     n_leaves=n_leaves, prediction_type=\"deterministic\",\n",
    "                                     random_state=0)\n",
    "HLR = HLR.fit(X_train.to_numpy(), y_train, cluster_labels=y_train, leaves_assignment=assignment)\n",
    "accuracy = HLR.score(X_test.to_numpy(), y_test)\n",
    "print(\"Accuracy using true labellling: {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8cf04021",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8796992481203008"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485b4f13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "decision_trees",
   "language": "python",
   "name": "decision_trees"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
